---
sidebar_position: 1
---

import CodeBlock from "@theme/CodeBlock";
import ExampleOllama from "@examples/ollama-completion-example/ollama_completion_example.go";

# 快速入门：LangChainGo 与 Ollama

通过运行您的第一个 LangChainGo 和 [Ollama](https://ollama.ai/) 程序来开始。Ollama 提供了在所有计算机平台上进行本地 LLM 推理的最直接方法。

## 前提条件

1.  **Ollama**：[下载并安装 Ollama](https://ollama.ai/)。
2.  **Go**：[下载并安装 Go](https://go.dev/doc/install)。

## 设置

Ollama 在您的本地计算机上运行，不需要 API 密钥。但是，您需要安装 Ollama 并下载一个模型。

### 安装 Ollama
请在 [ollama.ai](https://ollama.ai/) 上按照您操作系统的安装说明进行操作。

### 下载模型
在运行示例之前，您需要下载一个模型。该示例使用 `llama2` 模型：

```bash
ollama pull llama2
```

## 步骤

1.  **初始化 Ollama**：在您的终端中，执行命令 `ollama run llama2`。第一次运行可能需要一些时间，因为模型需要下载到您的计算机上。
2.  **运行示例**：输入命令：
    ```shell
    go run github.com/tmc/langchaingo/examples/ollama-completion-example@main
    ```

您应该会看到类似以下的输出：

```shell
第一个踏上月球的人是美国宇航员尼尔·阿姆斯特朗，他于1969年7月20日在阿波罗11号任务期间踏上了月球表面。
```
*(译者注：输出内容已翻译)*

恭喜！您已成功构建并执行了第一个使用本地推理的开源 LLM 程序。

这是完整的程序（来自 [ollama-completion-example](https://github.com/tmc/langchaingo/blob/main/examples/ollama-completion-example/ollama_completion_example.go)）：

<CodeBlock language="go">{ExampleOllama}</CodeBlock>
